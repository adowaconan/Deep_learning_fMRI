{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.3. demonstrate the DNN works.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adowaconan/Deep_learning_fMRI/blob/master/5_3_demonstrate_the_DNN_works.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ANw7YGV5OD5o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Just installing some of the necessary libraries to get the data and process the data"
      ]
    },
    {
      "metadata": {
        "id": "rHsjgJb2MgPv",
        "colab_type": "code",
        "outputId": "9bf58a2e-58e3-4be2-bc6f-170e86c2732c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1251
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install tqdm\n",
        "!apt-get install swig\n",
        "!pip install -U pymvpa2\n",
        "!pip install -U seaborn\n",
        "\n",
        "# this lets you access to googel drive shared files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python2.7/dist-packages (4.28.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 3s (329 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 110851 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Collecting pymvpa2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/5c/9020095e1180ab5fefb25ff9299c5a32696664cc9c74ed67ce19768f50f9/pymvpa2-2.6.5.tar.gz (8.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 8.1MB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python2.7/dist-packages (from pymvpa2) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: nibabel in /usr/local/lib/python2.7/dist-packages (from pymvpa2) (2.3.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from scipy->pymvpa2) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: bz2file; python_version < \"3.0\" in /usr/local/lib/python2.7/dist-packages (from nibabel->pymvpa2) (0.98)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.3 in /usr/local/lib/python2.7/dist-packages (from nibabel->pymvpa2) (1.11.0)\n",
            "Building wheels for collected packages: pymvpa2\n",
            "  Building wheel for pymvpa2 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f7/be/b2/857d1fc6d324a4e5499ac98fd45f79eaf9457265364c838851\n",
            "Successfully built pymvpa2\n",
            "Installing collected packages: pymvpa2\n",
            "Successfully installed pymvpa2-2.6.5\n",
            "Collecting seaborn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/bf/04cfcfc9616cedd4b5dd24dfc40395965ea9f50c1db0d3f3e52b050f74a5/seaborn-0.9.0.tar.gz (198kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 25.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.9.3 in /usr/local/lib/python2.7/dist-packages (from seaborn) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14.0 in /usr/local/lib/python2.7/dist-packages (from seaborn) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.15.2 in /usr/local/lib/python2.7/dist-packages (from seaborn) (0.22.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=1.4.3 in /usr/local/lib/python2.7/dist-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas>=0.15.2->seaborn) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python2.7/dist-packages (from pandas>=0.15.2->seaborn) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: backports.functools-lru-cache in /usr/local/lib/python2.7/dist-packages (from matplotlib>=1.4.3->seaborn) (1.5)\n",
            "Requirement already satisfied, skipping upgrade: subprocess32 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=1.4.3->seaborn) (3.5.3)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=1.4.3->seaborn) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=1.4.3->seaborn) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python2.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (40.7.0)\n",
            "Building wheels for collected packages: seaborn\n",
            "  Building wheel for seaborn (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fc/1c/74/c8f80a532c06a789599b8659b117ec7d7574cac4a06f7dabfe\n",
            "Successfully built seaborn\n",
            "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: seaborn\n",
            "  Found existing installation: seaborn 0.7.1\n",
            "    Uninstalling seaborn-0.7.1:\n",
            "      Successfully uninstalled seaborn-0.7.1\n",
            "Successfully installed seaborn-0.9.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "seaborn"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "4Js0klgZOK54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Copying the word list and the word2vec model from Google Drive to Colab, so it will take some time."
      ]
    },
    {
      "metadata": {
        "id": "2Q5s3nrIN3FZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "word_list_id = 'https://drive.google.com/open?id=18nfVy-o0GWX-QKEWrKK0EKLLAltpFy4U'.split('id=')[-1]\n",
        "word2vec_model_id = 'https://drive.google.com/open?id=1Dj9aTqHbuknWZC9kQJabqH3NqGXe_PT4'.split('id=')[-1]\n",
        "\n",
        "word_list = drive.CreateFile({'id':word_list_id})\n",
        "word2vec_model_ = drive.CreateFile({'id':word2vec_model_id})\n",
        "\n",
        "word_list.GetContentFile('{}'.format('word.npy'))\n",
        "word2vec_model_.GetContentFile('{}'.format('glove-sbwc.i25.vec'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "drDbxPrjQ2kd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "roi_id = 'https://drive.google.com/open?id=10cAoPizl69QR2RekIKpkdrv9J4_Emcq-'.split('id=')[-1]\n",
        "roi = drive.CreateFile({'id':roi_id})\n",
        "roi.GetContentFile('{}'.format('roi.pkl'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "buSM-QDVObIW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The python libraries we will need to import\n",
        "\n",
        "1. numpy: numerical python\n",
        "2. pandas: table\n",
        "3. tqdm: progress bar\n",
        "4. **gensim**: need for loading word2vec models\n",
        "5. scipy.spatial.distance: representational dissimilarity matrix\n",
        "6. MinMaxScaler: rescale features to between 0 and 1\n",
        "7. LinearSVC: a selected linear classifier\n",
        "8. roc_auc_score: a selected scoring method, good for balanced or unbalanced data\n",
        "9. CalibratedClassifierCV: a classifier wrapper for providing probabilistic predictions\n",
        "10. MultiOutputClassifier: a classifier wrapper for decoding if target is more than 1-D\n",
        "11. model_selection: CV method\n",
        "12. seaborn (better in 0.9.0 version): pretty plotting\n",
        "13. pickle: to load the dataset (PYMVPA format) that is stored in a pickle dump file"
      ]
    },
    {
      "metadata": {
        "id": "zt9i6rYmOam_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from gensim.models.keyedvectors import KeyedVectors # for loading word2vec models\n",
        "from scipy.spatial import distance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.model_selection import StratifiedShuffleSplit,LeavePGroupsOut\n",
        "import seaborn as sns\n",
        "sns.set_style('white')\n",
        "sns.set_context('poster')\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QOucIhX2M_uQ",
        "colab_type": "code",
        "outputId": "8f5347e4-cd52-4299-ae4b-a5a57c8084e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# candidate model\n",
        "Glove_from_SBWC = 'glove-sbwc.i25.vec'\n",
        "# load the stimuli table\n",
        "words = np.load('word.npy')\n",
        "print('loading Glove model, and it is going to take some time...')\n",
        "model_glove = KeyedVectors.load_word2vec_format(Glove_from_SBWC)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading Glove model, and it is going to take some time...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "16IljDioTxsA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# I lost track of how many keras functions I imported, so bear with me."
      ]
    },
    {
      "metadata": {
        "id": "sSvTVIGyPlUC",
        "colab_type": "code",
        "outputId": "b47df88b-bfb3-4104-c194-0c06a42d58ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers                                  import Dense,Dropout,BatchNormalization\n",
        "from keras.layers                                  import Input\n",
        "from keras.layers                                  import Flatten,LeakyReLU,merge\n",
        "from keras.layers                                  import GaussianNoise,GaussianDropout\n",
        "\n",
        "from keras.models                                  import Model,Sequential\n",
        "from keras.layers.noise                            import AlphaDropout\n",
        "from keras.callbacks                               import ModelCheckpoint,TensorBoard,EarlyStopping\n",
        "from keras                                         import backend as K\n",
        "from keras                                         import regularizers\n",
        "from keras.layers                                  import Reshape\n",
        "from keras.layers                                  import Conv3DTranspose,Activation, Lambda,Multiply\n",
        "from keras.layers                                  import Layer\n",
        "from keras.losses                                  import mse\n",
        "\n",
        "from mvpa2.datasets.base                           import Dataset\n",
        "from mvpa2.mappers.fx                              import mean_group_sample\n",
        "\n",
        "from sklearn.metrics                               import roc_auc_score,roc_curve\n",
        "from sklearn.metrics                               import (classification_report,\n",
        "                                                           matthews_corrcoef,\n",
        "                                                           confusion_matrix,\n",
        "                                                           f1_score,\n",
        "                                                           log_loss)\n",
        "from sklearn.preprocessing                         import MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection                       import StratifiedShuffleSplit,LeaveOneGroupOut\n",
        "from sklearn.preprocessing                         import OneHotEncoder"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "f5pa3u2YT4cM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tons of helper functions, so that I don't have to repeat myself"
      ]
    },
    {
      "metadata": {
        "id": "8fFZYVoyNVwu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_blocks(dataset__,label_map,key_type='labels'):\n",
        "    \"\"\"\n",
        "    # use ids, chunks,and labels to make unique blocks of the pre-average dataset, because I don't want to \n",
        "    # average the dataset until I actually want to, but at the same time, I want to balance the data for \n",
        "    # both the training and test set.\n",
        "    \"\"\"\n",
        "    ids                     = dataset__.sa.id.astype(int)\n",
        "    chunks                  = dataset__.sa.chunks\n",
        "    words                   = dataset__.sa.words\n",
        "    if key_type == 'labels':\n",
        "        try: # in metasema\n",
        "            labels              = np.array([label_map[item] for item in dataset__.sa.targets])[:,-1]\n",
        "        except:# not in metasema\n",
        "            labels              = np.array([label_map[item] for item in dataset__.sa.targets])\n",
        "        \n",
        "    elif key_type == 'words':\n",
        "        labels              = np.array([label_map[item] for item in dataset__.sa.words])\n",
        "    sample_indecies         = np.arange(len(labels))\n",
        "    blocks                  = [np.array([ids[ids             == target],\n",
        "                                         chunks[ids          == target],\n",
        "                                         words[ids           == target],\n",
        "                                         labels[ids          == target],\n",
        "                                         sample_indecies[ids == target]\n",
        "                                         ]) for target in np.unique(ids)]\n",
        "    block_labels            = np.array([np.unique(ll[-2]) for ll in blocks]).ravel()\n",
        "    return blocks,block_labels\n",
        "\n",
        "def customized_partition(dataset__,label_map):\n",
        "    \"\"\"\n",
        "    Similar to the function above, but it nests with cross validation process,\n",
        "    making sure each word instance will appear in the test set for at least once\n",
        "    \"\"\"\n",
        "    unique_words = np.unique(dataset__.sa.words)\n",
        "    unique_chunks = np.unique(dataset__.sa.chunks)\n",
        "    try: # in metasema\n",
        "        labels              = np.array([label_map[item] for item in dataset__.sa.targets])[:,-1]\n",
        "    except:# not in metasema\n",
        "        labels              = np.array([label_map[item] for item in dataset__.sa.targets])\n",
        "    words = dataset__.sa.words\n",
        "    chunks = dataset__.sa.chunks\n",
        "    blocks,block_labels = get_blocks(dataset__,label_map,key_type='labels')\n",
        "    sample_indecies         = np.arange(len(labels))\n",
        "    test = []\n",
        "    check = []\n",
        "    for n in range(int(1e5)):\n",
        "        random_chunk = np.random.choice(unique_chunks,size=1,replace=False)[0]\n",
        "        working_words = words[chunks == random_chunk]\n",
        "        working_block = [block for block in blocks if (int(np.unique(block[1])[0]) == random_chunk)]\n",
        "        random_word = np.random.choice(working_words,size=1,replace=False)[0]\n",
        "        if random_word not in check:\n",
        "            for block in working_block:\n",
        "                if (np.unique(block[2])[0] == random_word) and (random_word not in check):\n",
        "                    test.append(block[-1].astype(int))\n",
        "                    check.append(block[2][0])\n",
        "#                    print(test,check)\n",
        "                if len(check) == len(unique_words):\n",
        "                    break\n",
        "            if len(check) == len(unique_words):\n",
        "                break\n",
        "        if len(check) == len(unique_words):\n",
        "            break\n",
        "    test = np.concatenate(test,0).flatten()\n",
        "    train = np.array([idx for idx in sample_indecies if (idx not in test)])\n",
        "    return train,test\n",
        "\n",
        "class stacked_perceptron(object):\n",
        "    \"\"\"\n",
        "    to construct stacked layers of dense-batchnormaization-activation-layers\n",
        "    edit: batch normalization is no used due to activation SELU\n",
        "    https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9\n",
        "    https://arxiv.org/pdf/1706.02515.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "#                 inputs,\n",
        "                 hidden_units, # number of neurons of the dense layer\n",
        "                 layer_func                                 = Dense,\n",
        "                 beta                                       = 1, # L1 regularization value\n",
        "                 lamda                                      = 1,# L2 regularization, used exclusively in parameters regularization\n",
        "                 dropout_func                               = AlphaDropout,# special drop out function: https://arxiv.org/pdf/1706.02515.pdf\n",
        "                 dropout_rate                               = 0.2,# dropout rate\n",
        "                 name                                       = [1,1,'encode'],# the way of naming the layers\n",
        "                 kernel_regularizer                         = regularizers.l1_l2,# parameter regularization function: elatic\n",
        "                 kernel_initializer                         ='he_normal',# initialize the parameter method\n",
        "                 activation                                 = 'selu',# self normalizing NN\n",
        "                 alpha                                      = 0.3, # not used\n",
        "                 l1                                         = False, # default of not using L1 activation regularization\n",
        "                 use_bias                                   = True,# defult of using bias terms in the dense layers\n",
        "                 batch_normal                               = True,# defult of using batch normalization\n",
        "                 ):\n",
        "#        self.inputs = inputs\n",
        "        self.layer_func                                     = layer_func\n",
        "        self.hidden_units                                   = hidden_units\n",
        "        self.kernel_regularizer                             = kernel_regularizer\n",
        "        self.kernel_initializer                             = kernel_initializer\n",
        "        self.beta                                           = beta\n",
        "        self.lamda                                          = lamda\n",
        "        self.dropout_func                                   = dropout_func\n",
        "        self.dropout_rate                                   = dropout_rate\n",
        "        self.name                                           = name\n",
        "        self.activation                                     = activation\n",
        "        self.alpha                                          = alpha\n",
        "        self.l1                                             = l1\n",
        "        self.use_bias                                       = use_bias\n",
        "        self.batch_normal                                   = batch_normal\n",
        "    def __call__(self,net):\n",
        "        ####################### dense layer block ###################################################\n",
        "        if self.l1: # if regularize the activation output\n",
        "            net = self.layer_func(units                     = self.hidden_units,\n",
        "                        use_bias                            = self.use_bias,\n",
        "                        kernel_initializer                  = self.kernel_initializer,\n",
        "                        kernel_regularizer                  = self.kernel_regularizer(1e-6,self.lamda),\n",
        "                        activity_regularizer                = regularizers.l1(self.beta),\n",
        "                        name                                = 'dense_layer{}_{}_{}'.format(self.name[0],\n",
        "                                                                                           self.name[1],\n",
        "                                                                                           self.name[2])\n",
        "                        )(net)\n",
        "        else:\n",
        "            net = self.layer_func(units                     = self.hidden_units,\n",
        "                        use_bias                            = True,\n",
        "                        kernel_initializer                  = self.kernel_initializer,\n",
        "                        kernel_regularizer                  = self.kernel_regularizer(1e-6,self.lamda),\n",
        "#                        activity_regularizer = regularizers.l1(self.beta),\n",
        "                        name                                = 'dense_layer{}_{}_{}'.format(self.name[0],\n",
        "                                                                                           self.name[1],\n",
        "                                                                                           self.name[2])\n",
        "                        )(net)\n",
        "        #################### end of dense layer block #################################################\n",
        "        #################### batch normalization block ################################################\n",
        "        if self.batch_normal:\n",
        "            net = BatchNormalization(name                   = 'norm_layer{}_{}_{}'.format(self.name[0],\n",
        "                                                                                          self.name[1]+1,\n",
        "                                                                                          self.name[2]))(net)\n",
        "        ################### end of batch normalization block ################################################\n",
        "        ################### actiavtion block ##########################################################\n",
        "        if type(self.activation) is not str:\n",
        "            net = self.activation(name                      = 'activation_layer{}_{}_{}'.format(self.name[0],\n",
        "                                                                                                self.name[1]+2, \n",
        "                                                                                                self.name[2]),\n",
        "                                  alpha                     = self.alpha)(net)\n",
        "        else:\n",
        "            net = Activation(activation = self.activation,\n",
        "                             name                           = 'activation_layer{}_{}_{}'.format(self.name[0],\n",
        "                                                                                                self.name[1]+2, \n",
        "                                                                                                self.name[2]),\n",
        "                              )(net)\n",
        "        ################### end of activation block ######################################################\n",
        "        ################### dropout layer block ##########################################################\n",
        "        net = self.dropout_func(rate                        = self.dropout_rate,\n",
        "                      name                                  = 'drop_layer{}_{}_{}'.format(self.name[0],\n",
        "                                                                                          self.name[1]+3,\n",
        "                                                                                          self.name[2]))(net)\n",
        "        \n",
        "        return net\n",
        "def make_CallBackList(model_name,monitor='val_loss',mode='min',verbose=0,min_delta=1e-4,patience=50,frequency = 1):\n",
        "    \"\"\"\n",
        "    Make call back function lists for the keras models\n",
        "    \n",
        "    Inputs\n",
        "    -------------------------\n",
        "    model_name: directory of where we want to save the model and its name\n",
        "    monitor: the criterion we used for saving or stopping the model\n",
        "    mode: min --> lower the better, max --> higher the better\n",
        "    verboser: printout the monitoring messages\n",
        "    min_delta: minimum change for early stopping\n",
        "    patience: temporal windows of the minimum change monitoring\n",
        "    frequency: temporal window steps of the minimum change monitoring\n",
        "    \n",
        "    Return\n",
        "    --------------------------\n",
        "    CheckPoint: saving the best model\n",
        "    EarlyStopping: early stoppi....\n",
        "    \"\"\"\n",
        "    checkPoint = ModelCheckpoint(model_name,# saving path\n",
        "                                 monitor          =monitor,# saving criterion\n",
        "                                 save_best_only   =True,# save only the best model\n",
        "                                 mode             =mode,# saving criterion\n",
        "                                 period           =frequency,# frequency of check the update \n",
        "                                 verbose          =verbose# print out (>1) or not (0)\n",
        "                                 )\n",
        "    earlyStop = EarlyStopping(   monitor          =monitor,\n",
        "                                 min_delta        =min_delta,\n",
        "                                 patience         =patience,\n",
        "                                 verbose          =verbose, \n",
        "                                 mode             =mode,\n",
        "                                 )\n",
        "    return [checkPoint,earlyStop]\n",
        "def clf(shape,\n",
        "         beta               = 1,\n",
        "         lamda              = 1,\n",
        "         dropout_rate       = 0.,\n",
        "         latent_dim         = 36,\n",
        "         lr                 = 1e-3,\n",
        "         decay              = 0,\n",
        "         add_noise          = False,\n",
        "         add_sparsity       = False,\n",
        "         rho                = 0.02,\n",
        "         sparse_beta        = 1,\n",
        "         output_shape       = 2,\n",
        "         \n",
        "         ):\n",
        "    inputs                  = Input(shape                                   = (shape,),\n",
        "                              batch_shape                                   = (None,shape),\n",
        "                              name                                          = 'inputs',\n",
        "                              dtype                                         = 'float32'\n",
        "                              )\n",
        "    ####################################### add noise to the inputs #################################\n",
        "    ############################ Gaussian noise #####################################################\n",
        "    if add_noise:\n",
        "        inputs_            = GaussianNoise(stddev = 1)(inputs)\n",
        "        encode_            = inputs_\n",
        "    else:\n",
        "        encode_            = inputs\n",
        "    if type(latent_dim) is int:\n",
        "        latent_dims = [latent_dim]\n",
        "    else:\n",
        "        latent_dims = latent_dim\n",
        "    for jj,latent_dim_ in enumerate(latent_dims):\n",
        "        np.random.seed(12345)\n",
        "        encode_           = stacked_perceptron(latent_dim_,\n",
        "                                              beta                          = beta,\n",
        "                                              lamda                         = lamda,\n",
        "                                              dropout_rate                  = dropout_rate,\n",
        "                                              name                          = [jj+1,jj+1,'latent'],\n",
        "                                              use_bias                      = True,\n",
        "                                              batch_normal                  = True,\n",
        "                                              activation                    = 'sigmoid',\n",
        "                                              )(encode_)\n",
        "        if add_sparsity:\n",
        "            encode_       = _sparse_reg(rho = rho, beta = sparse_beta)(encode_)\n",
        "    decode_                = encode_\n",
        "    np.random.seed(12345)\n",
        "    outputs                = Dense(units                                    = output_shape,\n",
        "                             use_bias                                       = True,\n",
        "                             name                                           ='outputs',\n",
        "                             kernel_initializer                             = 'he_normal',\n",
        "                             kernel_regularizer                             = regularizers.l1_l2(beta,lamda),\n",
        "                             activity_regularizer                           = regularizers.l1(1),\n",
        "                             activation                                     = 'softmax',# constraint between 0 and 1\n",
        "                             )(decode_)\n",
        "    model                  = Model(inputs, outputs, name = 'classifier')\n",
        "    model.compile(optimizer= keras.optimizers.Adam(lr = lr,decay = decay),\n",
        "                  loss     = keras.losses.categorical_crossentropy,\n",
        "                  metrics  = [keras.metrics.categorical_accuracy]\n",
        "                  )\n",
        "    word_outputs = Dense(units                                          = latent_dim,\n",
        "                         use_bias                                       = True,\n",
        "                         name                                           ='outputs',\n",
        "                         kernel_initializer                             = 'he_normal',\n",
        "                         kernel_regularizer                             = regularizers.l1_l2(beta,lamda),\n",
        "                         activity_regularizer                           = regularizers.l1(1),\n",
        "                         activation                                     = 'softmax',# constraint between 0 and 1\n",
        "                         )(decode_)\n",
        "    word_model = Model(inputs,word_outputs,name = 'word_classifier')\n",
        "    word_model.compile(optimizer = keras.optimizers.Adam(lr = lr / 10., decay = decay * 1000.),\n",
        "                       loss      = keras.losses.categorical_crossentropy,\n",
        "                       metrics   = [keras.metrics.categorical_accuracy]\n",
        "                       )\n",
        "    return model,word_model\n",
        "def DNN_train_validate(dataset_train,\n",
        "                       dataset_test,\n",
        "                       label_map,\n",
        "                       output_dir,\n",
        "                       sub_name,\n",
        "                       roi_name,\n",
        "                       fold,\n",
        "                       patience,\n",
        "                       batch_size,\n",
        "                       epochs,\n",
        "                       print_train,\n",
        "                       classifier,\n",
        "                       flip = False,\n",
        "                       model_type = 'DNN',\n",
        "                       n_splits = 5,\n",
        "                ):\n",
        "    tr = dataset_train\n",
        "    X_train = tr.samples.astype('float32')\n",
        "    groups = tr.sa.chunks\n",
        "    te = dataset_test\n",
        "    te = te.get_mapped(mean_group_sample(['chunks', 'id'],order = 'occurrence'))\n",
        "    X_test = te.samples.astype('float32')\n",
        "    print('train on {} and test on {}'.format(tr.shape[0],te.shape[0]))\n",
        "    y_train = np.array([label_map[item] for item in tr.targets])\n",
        "    y_test = np.array([label_map[item] for item in te.targets])\n",
        "    if flip:\n",
        "        X_train_flip = np.fliplr(X_train)\n",
        "        X_train = np.concatenate([X_train,X_train_flip])\n",
        "        y_train = np.concatenate([y_train,y_train])\n",
        "        groups = np.concatenate([groups,groups])\n",
        "    \n",
        "    \n",
        "    # define the scaler\n",
        "    scaler          = MinMaxScaler() # scale the data between 0 and 1\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    onehot = OneHotEncoder()\n",
        "    if y_train.shape[-1] == 2:\n",
        "        labels_train = y_train\n",
        "        labels_test = y_test\n",
        "    else:\n",
        "        labels_train = onehot.fit_transform(y_train.reshape(-1,1)).toarray()\n",
        "        labels_test = onehot.transform(y_test.reshape(-1,1)).toarray()\n",
        "    \n",
        "    \n",
        "    # define the model name and saving directory\n",
        "    dnn_model_name   = 'temp.hdf5' # temporal model name \n",
        "    # we will need 2 call back functions: 1) in-train saving and 2) early stopping\n",
        "    callBackList = make_CallBackList(dnn_model_name,\n",
        "                                           monitor                      = 'val_{}'.format(classifier.metrics_names[-1]), # metric I care\n",
        "                                           mode                         = 'max', # how I care\n",
        "                                           verbose                      = 0,# print out the process\n",
        "                                           min_delta                    = 1e-5,# how much difference it should make\n",
        "                                           patience                     = patience, # early stop argument\n",
        "                                           frequency                    = 1\n",
        "                                           )\n",
        "    # first, feed the autoencoder 0s, and see what the raw-raw gradient values are\n",
        "    # a trick I learned from twitter\n",
        "    print('initial loss {:.4f}'.format(classifier.evaluate(np.zeros(X_train.shape),\n",
        "                                                        np.zeros((X_train.shape[0],labels_train.shape[1])),\n",
        "                                                        batch_size  = batch_size,\n",
        "                                                        verbose     = 0\n",
        "                                                        )[0]\n",
        "                                    )\n",
        "                    )\n",
        "    \n",
        "    blocks_tr,block_labels    = get_blocks(dataset_train,label_map)\n",
        "    cv = StratifiedShuffleSplit(n_splits=n_splits,test_size=.2,random_state=12345)\n",
        "    iterator = cv.split(blocks_tr,block_labels)\n",
        "    for z,(training,validation) in enumerate(iterator):\n",
        "        idx_training    = np.concatenate([blocks_tr[tr_el][-1] for tr_el in training]).astype(int)\n",
        "        idx_validation  = np.concatenate([blocks_tr[te_el][-1] for te_el in validation]).astype(int)\n",
        "        print('{} loss starts with {:.4f} on source'.format(model_type,\n",
        "                            classifier.evaluate(\n",
        "                                              X_train,\n",
        "                                              labels_train,\n",
        "                                              batch_size                    = batch_size,\n",
        "                                              verbose                       = 0\n",
        "                                                                      )[0]))\n",
        "        from sklearn import utils as sk_utils\n",
        "        X,y = sk_utils.shuffle(X_train[idx_training],labels_train[idx_training])\n",
        "        history                 = classifier.fit(  \n",
        "                                        X,y,# input, output\n",
        "                                        batch_size                              = batch_size,# min batch size\n",
        "                                        epochs                                  = epochs,# just lots of epochs\n",
        "                                        validation_data                         = (X_train[idx_validation],labels_train[idx_validation]),# split the last 10% of the data as the validation data\n",
        "                                        verbose                                 = print_train,# print out the process\n",
        "                                        callbacks                               = callBackList # call back functions\n",
        "                                        )\n",
        "\n",
        "        classifier.load_weights(dnn_model_name) # load the saved best model weights\n",
        "        # let's look at the current model loss with all the training + validation data combined\n",
        "        training_loss = classifier.evaluate(X_train,labels_train,\n",
        "                                             batch_size                     = batch_size,\n",
        "                                             verbose                        = 0\n",
        "                                             )\n",
        "        print('{} current loss = {:.4f},{} = {:.4f} on source'.format(\n",
        "              model_type,\n",
        "              training_loss[0],\n",
        "              classifier.metrics_names[-1],\n",
        "              training_loss[-1]))\n",
        "    \n",
        "    classifier.load_weights(dnn_model_name)\n",
        "    return (classifier,\n",
        "            scaler,\n",
        "            onehot,\n",
        "            dnn_model_name,\n",
        "            history,\n",
        "#            (score_baseline1,score_baseline2,score_baseline3,score_baseline4),\n",
        "#            (tn,fp,fn,tp),\n",
        "            X_train,\n",
        "            labels_train,\n",
        "            X_test,\n",
        "            labels_test,\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dDir6UmsR5I-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_map               = dict(animal  =[0,1],\n",
        "                               tool    =[1,0])\n",
        "sub                     = '*'       # star means all subjects\n",
        "average                 = False      # averaging the trainig data\n",
        "transfer                = False     # do I do domain adaptation\n",
        "concatenate             = False     # specifically for domain adaptation\n",
        "flip                    = True,     # double the size of the training data\n",
        "n_splits                = 10        # number of cross validation\n",
        "test_size               = 0.2       # proprotion of the test data\n",
        "patience                = 70 # if after this many epochs, the loss or the accuracy does not change, we stop the training\n",
        "epochs                  = 3000 # just use a large number to prompt early stop\n",
        "print_train             = 0 # 1 - print the progress bar, 0 - don't\n",
        "batch_size              = 64\n",
        "beta                    = 0.\n",
        "lamda                   = 0.\n",
        "learning_rate           = 1e-4\n",
        "decay                   = 1e-9\n",
        "dropout_rate            = 0.5\n",
        "latent_dim              = 36"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zPSRqOFgPmHW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ds = pickle.load(open('roi.pkl','rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K7t_fo8ARUJN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "condition = 'read'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WSZJdGNFRVDA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = ds[ds.sa.context == condition]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iKwr6cfZRcsJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "blocks, block_labels    = get_blocks(dataset,label_map)\n",
        "labels                  = np.array([label_map[item] for item in dataset.targets])\n",
        "cv                      = StratifiedShuffleSplit(n_splits       = n_splits,\n",
        "                                                 test_size      = test_size,\n",
        "                                                 random_state   = 12345)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FPchFaq-SDk0",
        "colab_type": "code",
        "outputId": "56c9e85e-5f33-4222-9cee-2b64fe1a77db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2322
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(12345)\n",
        "chunks = dataset.sa.chunks\n",
        "used_test = []\n",
        "fold = -1\n",
        "a = []\n",
        "output_dir = ''\n",
        "sub_name = 'test'\n",
        "roi_name = 'roi'\n",
        "for _ in range(1000):\n",
        "    print('paritioning ...')\n",
        "    idx_train,idx_test = customized_partition(dataset,label_map,)\n",
        "    if any([np.sum(idx_test) == np.sum(item) for item in used_test]):\n",
        "        pass\n",
        "    else:\n",
        "        fold += 1\n",
        "        used_test.append(idx_test)\n",
        "        print('done')\n",
        "        \n",
        "        \n",
        "        # must do!!! Otherwise, it will run out of resource eventualy\n",
        "        K.clear_session()\n",
        "        shape       = dataset.shape[1]\n",
        "        # control the initialization of the DNN model\n",
        "        np.random.seed(12345)\n",
        "        classifier,_= clf(\n",
        "                                  shape, # input shape\n",
        "                                  dropout_rate          = dropout_rate, # drop out rate for each hidden layer\n",
        "                                  beta                  = beta, # for sparsity\n",
        "                                  lamda                 = lamda, # for model simplicity\n",
        "                                  lr                    = learning_rate, # learning rate\n",
        "                                  decay                 = decay, # schedule decay of the learning rate \n",
        "                                  latent_dim            = latent_dim, # latent space dimensionality \n",
        "                                  )\n",
        "\n",
        "        (classifier,\n",
        "        scaler,\n",
        "        onehot,\n",
        "        dnn_model_name,\n",
        "        history,\n",
        "        X_train,\n",
        "        labels_train,\n",
        "        X_test,\n",
        "        labels_test,\n",
        "        )                        = DNN_train_validate(\n",
        "                                        dataset_train   = dataset[idx_train],\n",
        "                                        dataset_test    = dataset[idx_test],\n",
        "                                        classifier      = classifier,\n",
        "                                        label_map       = label_map,\n",
        "                                        output_dir      = output_dir,\n",
        "                                        sub_name        = sub_name,\n",
        "                                        roi_name        = roi_name,\n",
        "                                        fold            = fold,\n",
        "                                        patience        = patience,\n",
        "                                        batch_size      = batch_size,\n",
        "                                        epochs          = epochs,\n",
        "                                        print_train     = print_train,\n",
        "                                        flip            = True,\n",
        "                                        n_splits        = 5,\n",
        "                                        )\n",
        "        pred_                   = classifier.predict(X_test)\n",
        "        score_baseline1         = np.array([roc_auc_score(labels_test[:,ii],pred_[:,ii]) for ii in range(labels_test.shape[-1])])\n",
        "        a.append(score_baseline1.mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "paritioning ...\n",
            "done\n",
            "WARNING: There were no samples for combination {'chunks': 0, 'id': '111'}. It might be a sign of a disbalanced dataset <Dataset: 221x2927@float32, <sa: categories,chunks,context,id,targets,time_coords,time_indices,trials,words>, <fa: voxel_indices>, <a: imgaffine,imghdr,imgtype,mapper,voxel_dim,voxel_eldim>>.\n",
            " * Please note: warnings are printed only once, but underlying problem might occur many times *\n",
            "train on 1323 and test on 36\n",
            "initial loss 63.6531\n",
            "DNN loss starts with 64.5410 on source\n",
            "DNN current loss = 64.3109,categorical_accuracy = 0.6531 on source\n",
            "DNN loss starts with 64.3109 on source\n",
            "DNN current loss = 64.2384,categorical_accuracy = 0.6803 on source\n",
            "DNN loss starts with 64.2384 on source\n",
            "DNN current loss = 64.2384,categorical_accuracy = 0.6803 on source\n",
            "DNN loss starts with 64.2384 on source\n",
            "DNN current loss = 64.2346,categorical_accuracy = 0.6852 on source\n",
            "DNN loss starts with 64.2346 on source\n",
            "DNN current loss = 64.2291,categorical_accuracy = 0.6882 on source\n",
            "paritioning ...\n",
            "done\n",
            "train on 1323 and test on 36\n",
            "initial loss 63.6531\n",
            "DNN loss starts with 64.5455 on source\n",
            "DNN current loss = 64.2501,categorical_accuracy = 0.7173 on source\n",
            "DNN loss starts with 64.2501 on source\n",
            "DNN current loss = 64.1943,categorical_accuracy = 0.7249 on source\n",
            "DNN loss starts with 64.1943 on source\n",
            "DNN current loss = 64.1897,categorical_accuracy = 0.7358 on source\n",
            "DNN loss starts with 64.1897 on source\n",
            "DNN current loss = 64.1897,categorical_accuracy = 0.7358 on source\n",
            "DNN loss starts with 64.1897 on source\n",
            "DNN current loss = 64.2266,categorical_accuracy = 0.7230 on source\n",
            "paritioning ...\n",
            "done\n",
            "train on 1321 and test on 36\n",
            "initial loss 63.6889\n",
            "DNN loss starts with 64.5745 on source\n",
            "DNN current loss = 64.3092,categorical_accuracy = 0.6397 on source\n",
            "DNN loss starts with 64.3092 on source\n",
            "DNN current loss = 64.2993,categorical_accuracy = 0.6408 on source\n",
            "DNN loss starts with 64.2993 on source\n",
            "DNN current loss = 64.3219,categorical_accuracy = 0.6416 on source\n",
            "DNN loss starts with 64.3219 on source\n",
            "DNN current loss = 64.2699,categorical_accuracy = 0.6802 on source\n",
            "DNN loss starts with 64.2699 on source\n",
            "DNN current loss = 64.2755,categorical_accuracy = 0.6787 on source\n",
            "paritioning ...\n",
            "done\n",
            "train on 1322 and test on 36\n",
            "initial loss 63.6694\n",
            "DNN loss starts with 64.5535 on source\n",
            "DNN current loss = 64.3342,categorical_accuracy = 0.6044 on source\n",
            "DNN loss starts with 64.3342 on source\n",
            "DNN current loss = 64.2854,categorical_accuracy = 0.6320 on source\n",
            "DNN loss starts with 64.2854 on source\n",
            "DNN current loss = 64.2682,categorical_accuracy = 0.6566 on source\n",
            "DNN loss starts with 64.2682 on source\n",
            "DNN current loss = 64.2845,categorical_accuracy = 0.6543 on source\n",
            "DNN loss starts with 64.2845 on source\n",
            "DNN current loss = 64.2675,categorical_accuracy = 0.6630 on source\n",
            "paritioning ...\n",
            "done\n",
            "train on 1324 and test on 36\n",
            "initial loss 63.6397\n",
            "DNN loss starts with 64.5285 on source\n",
            "DNN current loss = 64.9524,categorical_accuracy = 0.5038 on source\n",
            "DNN loss starts with 64.9524 on source\n",
            "DNN current loss = 64.4059,categorical_accuracy = 0.5540 on source\n",
            "DNN loss starts with 64.4059 on source\n",
            "DNN current loss = 64.2763,categorical_accuracy = 0.6197 on source\n",
            "DNN loss starts with 64.2763 on source\n",
            "DNN current loss = 64.2788,categorical_accuracy = 0.6276 on source\n",
            "DNN loss starts with 64.2788 on source\n",
            "DNN current loss = 64.2662,categorical_accuracy = 0.6318 on source\n",
            "paritioning ...\n",
            "done\n",
            "train on 1325 and test on 36\n",
            "initial loss 63.6294\n",
            "DNN loss starts with 64.5154 on source\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-84f7d692c4ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m                                         \u001b[0mprint_train\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mprint_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                                         \u001b[0mflip\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                                         \u001b[0mn_splits\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         )\n\u001b[1;32m     60\u001b[0m         \u001b[0mpred_\u001b[0m                   \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-df36bbdf79e8>\u001b[0m in \u001b[0;36mDNN_train_validate\u001b[0;34m(dataset_train, dataset_test, label_map, output_dir, sub_name, roi_name, fold, patience, batch_size, epochs, print_train, classifier, flip, model_type, n_splits)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                         \u001b[0mvalidation_data\u001b[0m                         \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_validation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_validation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m# split the last 10% of the data as the validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                         \u001b[0mverbose\u001b[0m                                 \u001b[0;34m=\u001b[0m \u001b[0mprint_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m# print out the process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                         \u001b[0mcallbacks\u001b[0m                               \u001b[0;34m=\u001b[0m \u001b[0mcallBackList\u001b[0m \u001b[0;31m# call back functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m                                         )\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "gzkmCdzpS_YI",
        "colab_type": "code",
        "outputId": "705497d6-ac88-4d67-802a-07b3be4ccce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.mean(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5166666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "3L_-3WsyWaWi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}