{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10.1.searchlight representational similarity analysis, bonus - decoding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsEx4x2cm9AoNavWZIvNjk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmningmei/Deep_learning_fMRI_EEG/blob/master/10_1_searchlight_representational_similarity_analysis%2C_bonus_decoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df-vPFlvQ8p3"
      },
      "source": [
        "# The script to illustrate a way to perform searchlight RSA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXrHGp7VSWEv"
      },
      "source": [
        "# Get the extracted features and the mask files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzt5iZvKPx0H",
        "outputId": "e68a1b94-d567-4fbe-db33-ec49800733b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try:\n",
        "    !git clone https://github.com/nmningmei/Extracted_features_of_Spanish_image_dataset.git\n",
        "except:\n",
        "    !ls Extracted_features_of_Spanish_image_dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Extracted_features_of_Spanish_image_dataset' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaBuTFDoZQTv"
      },
      "source": [
        "# Get the fMRI data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN2ilE3MYPWj"
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1vLWSdXpOxqp3jOCypsWA27AEptsGdwav'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siP40EAcRdO_"
      },
      "source": [
        "downloaded.GetContentFile('whole_bran.tar.gz')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t4uX01wY6aU",
        "outputId": "e3b2ef0f-6f00-45bc-b124-bca00060f8ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!tar -xvf whole_bran.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "whole_brain_conscious.csv\n",
            "whole_brain_unconscious.csv\n",
            "whole_brain_unconscious.nii.gz\n",
            "whole_brain_conscious.nii.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2olG3v16ZKVC",
        "outputId": "8e0bc03a-05d6-4db0-a72a-46f23ea0320d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json                                      \u001b[0m\u001b[01;32mwhole_brain_conscious.nii.gz\u001b[0m*\n",
            "\u001b[01;34mExtracted_features_of_Spanish_image_dataset\u001b[0m/  \u001b[01;32mwhole_brain_unconscious.csv\u001b[0m*\n",
            "RSA_unconscious.nii.gz                        \u001b[01;32mwhole_brain_unconscious.nii.gz\u001b[0m*\n",
            "\u001b[01;34msample_data\u001b[0m/                                  whole_bran.tar.gz\n",
            "\u001b[01;32mwhole_brain_conscious.csv\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAz9nOmO_-I5"
      },
      "source": [
        "# install and update some of the libraries if not "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Rkj9z1ZOc0",
        "outputId": "7813d44d-ee0e-41b0-abf8-a5cc3379127a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try:\n",
        "    from nilearn.input_data import NiftiMasker\n",
        "    from nilearn.image import new_img_like\n",
        "    from brainiak.searchlight.searchlight import Searchlight\n",
        "    from brainiak.searchlight.searchlight import Ball\n",
        "except:\n",
        "    !pip install nilearn\n",
        "    !python3 -m pip install -U brainiak\n",
        "    from nilearn.input_data import NiftiMasker\n",
        "    from nilearn.image import new_img_like\n",
        "    from brainiak.searchlight.searchlight import Searchlight\n",
        "    from brainiak.searchlight.searchlight import Ball\n",
        "\n",
        "import os,gc\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nibabel import load as load_fmri\n",
        "from joblib  import Parallel,delayed\n",
        "from scipy.spatial import distance\n",
        "from scipy.stats import spearmanr"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpT4JwEPlXV5"
      },
      "source": [
        "def normalize(data,axis = 1):\n",
        "    return data - data.mean(axis).reshape(-1,1)\n",
        "# Define voxel function\n",
        "def sfn(l, msk, myrad, bcast_var):\n",
        "    \"\"\"\n",
        "    l: BOLD\n",
        "    msk: mask array\n",
        "    myrad: not use\n",
        "    bcast_var: label -- CNN features\n",
        "    \"\"\"\n",
        "    BOLD = l[0][msk,:].T.copy() # vectorize the voxel values in the sphere\n",
        "    model = bcast_var.copy() # vectorize the RDM\n",
        "    # pearson correlation\n",
        "    RDM_X   = distance.pdist(normalize(BOLD),'correlation')\n",
        "    RDM_y   = distance.pdist(normalize(model),'correlation')\n",
        "    D,p     = spearmanr(RDM_X,RDM_y)\n",
        "    return D\n",
        "def process_csv(file_name = 'whole_brain_conscious.csv'):\n",
        "    \"\"\"\n",
        "    to add some info to the event files to create better cross-validation folds\n",
        "    \"\"\"\n",
        "    df_data = pd.read_csv(file_name)\n",
        "    df_data['id'] = df_data['session'] * 1000 + df_data['run'] * 100 + df_data['trials']\n",
        "    df_data = df_data[df_data.columns[1:]]\n",
        "    return df_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upBOZVVYnfDO"
      },
      "source": [
        "radius              = 3 # in mm, the data has voxel size of 2.4mm x 2.4mm x 2.4mm\n",
        "feature_dir         = 'Extracted_features_of_Spanish_image_dataset/computer_vision_features_no_background'\n",
        "model_name          = 'VGG19'\n",
        "label_map           = {'Nonliving_Things':[0,1],'Living_Things':[1,0]}\n",
        "whole_brain_mask    = 'Extracted_features_of_Spanish_image_dataset/combine_BOLD.nii.gz'\n",
        "average             = True\n",
        "n_splits            = 10 # recommend to perform the resampling for more than 500 times\n",
        "n_jobs              = -1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuOAEFxThxXc"
      },
      "source": [
        "# implementation of resampling\n",
        "\n",
        "1. when the study is event-related but the events are not balanced, distribution is hard to measure, so resampling could avoid handling the assumption of normal distribution\n",
        "2. by the law of large number, when the time of resampling is large enough (exhaust the permutation, i.e. 1-2-3, 2-3-1, 3-1-2, 1-3-2, 2-1-3, 3-2-1, or just a large number, like n = 1000), the average of the resampling estimate is an unbias estimate of the population measure, and the 95% credible interval (confidence interval for frequentist statitistics) contains the population with chance of 95%.\n",
        "3. so, we gather one trial of each unique item, in total 96 trial, for both the BOLD signals (96, 88, 88, 66) and the CNN features (96, 300), and we compute the RDM of the BOLD signals in the searchlight sphere and the RDM of the CNN features regardless of the searchlight sphere. We then correlate the RDM of the BOLD in the searchlight sphere to the RDM of the CNN features.\n",
        "4. repeat step 3. for different sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kScQ0txinypn",
        "outputId": "9a8c0ba2-8447-4af6-f984-0f59ad1c2d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for conscious_state in ['unconscious','conscious']:\n",
        "    np.random.seed(12345)\n",
        "\n",
        "    df_data         = process_csv(f'whole_brain_{conscious_state}.csv')\n",
        "    # load the data in the format of numpy but keep the 4D dimensions\n",
        "    BOLD_image      = load_fmri(f'whole_brain_{conscious_state}.nii.gz')\n",
        "    print(f'{conscious_state}\\tfMRI in {BOLD_image.shape} events in {df_data.shape}')\n",
        "    targets         = np.array([label_map[item] for item in df_data['targets']])[:,-1]\n",
        "    # get the image names in the order of the experimental trials\n",
        "    images          = df_data['paths'].apply(lambda x: x.split('.')[0] + '.npy').values\n",
        "    # get the CNN features (n_trial x 300)\n",
        "    CNN_feature     = np.array([np.load(os.path.join(feature_dir,\n",
        "                                                     model_name,\n",
        "                                                     item)) for item in images])\n",
        "    groups          = df_data['labels'].values\n",
        "\n",
        "    # define a function to create the folds first\n",
        "    def _proc(df_data):\n",
        "        \"\"\"\n",
        "        This is useful when the number of folds are thousands\n",
        "        \"\"\"\n",
        "        df_picked = df_data.groupby('labels').apply(lambda x: x.sample(n = 1).drop('labels',axis = 1)).reset_index()\n",
        "        df_picked = df_picked.sort_values(['targets','subcategory','labels'])\n",
        "        idx_test  = df_picked['level_1'].values\n",
        "        return idx_test\n",
        "    print(f'partitioning data for {n_splits} folds')\n",
        "    idxs = Parallel(n_jobs = -1, verbose = 1)(delayed(_proc)(**{\n",
        "                'df_data':df_data,}) for _ in range(n_splits))\n",
        "    gc.collect() # free memory that is occupied by garbage\n",
        "\n",
        "    # define a function to run the RSA\n",
        "    def _searchligh_RSA(idx,\n",
        "                        sl_rad = radius, \n",
        "                        max_blk_edge = radius - 1, \n",
        "                        shape = Ball,\n",
        "                        min_active_voxels_proportion = 0,\n",
        "                        ):\n",
        "        # Brainiak function\n",
        "        sl = Searchlight(sl_rad = sl_rad, \n",
        "                        max_blk_edge = max_blk_edge, \n",
        "                        shape = shape,\n",
        "                        min_active_voxels_proportion = min_active_voxels_proportion,\n",
        "                        )\n",
        "        # distribute the data based on the sphere\n",
        "        ## the first input is usually the BOLD signal, and it is in the form of \n",
        "        ## lists not arrays, representing each subject\n",
        "        ## the second input is usually the mask, and it is in the form of array\n",
        "        sl.distribute([np.asanyarray(BOLD_image.dataobj)[:,:,:,idx]], \n",
        "                        np.asanyarray(load_fmri(whole_brain_mask).dataobj) == 1)\n",
        "        # broadcasted data is the data that remains the same during RSA\n",
        "        sl.broadcast(CNN_feature[idx])\n",
        "        # run searchlight algorithm\n",
        "        global_outputs = sl.run_searchlight(sfn,\n",
        "                                            pool_size = 1, # we run each RSA using a single CPU\n",
        "                                            )\n",
        "        return global_outputs\n",
        "    for _ in range(10):\n",
        "        gc.collect()\n",
        "    res = Parallel(n_jobs = -1,verbose = 1,)(delayed(_searchligh_RSA)(**{\n",
        "                'idx':idx}) for idx in idxs)\n",
        "    # save the data\n",
        "    results_to_save = np.zeros(np.concatenate([BOLD_image.shape[:3],[n_splits]]))\n",
        "    for ii,item in enumerate(res):\n",
        "        results_to_save[:,:,:,ii] = np.array(item, dtype=np.float)\n",
        "    results_to_save = new_img_like(BOLD_image,results_to_save,)\n",
        "    results_to_save.to_filename(f'RSA_{conscious_state}.nii.gz')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unconscious\tfMRI in (88, 88, 66, 782) events in (782, 20)\n",
            "partitioning data for 10 folds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    3.0s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 18.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "conscious\tfMRI in (88, 88, 66, 514) events in (514, 20)\n",
            "partitioning data for 10 folds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.7s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgSZDnr1anO9"
      },
      "source": [
        "# let's modify the code above to make it a searchlight decoding, without using nilearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGIC_Oc1b6F4"
      },
      "source": [
        "# modify the voxel function for decoding\n",
        "# like defining the decoder\n",
        "# like putting the train-test pipeline in place\n",
        "def sfn(l, msk, myrad, bcast_var):\n",
        "    \"\"\"\n",
        "    l: BOLD\n",
        "    msk: mask array\n",
        "    myrad: not use\n",
        "    bcast_var: label \n",
        "    \"\"\"\n",
        "    from sklearn.svm import LinearSVC\n",
        "    from sklearn.calibration import CalibratedClassifierCV\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    from sklearn.pipeline import make_pipeline\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    BOLD = l[0][msk,:].T.copy() # vectorize the voxel values in the sphere\n",
        "    targets,idx_train,idx_test = bcast_var\n",
        "    \n",
        "    # scaler the data to between 0 and 1, improve SVM decoding\n",
        "    scaler = MinMaxScaler()\n",
        "    # default with L2 regularization\n",
        "    svm = LinearSVC(class_weight = 'balanced',random_state = 12345)\n",
        "    # make the SVM to produce probabilistic predictions \n",
        "    svm = CalibratedClassifierCV(svm, cv = 5)\n",
        "    pipeline = make_pipeline(scaler,svm)\n",
        "    pipeline.fit(BOLD[idx_train],targets[idx_train])\n",
        "    y_pred = pipeline.predict_proba(BOLD[idx_test])[:,-1]\n",
        "    score = roc_auc_score(targets[idx_test],y_pred)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7CSwuBND7DC"
      },
      "source": [
        "for conscious_state in ['unconscious','conscious']:\n",
        "    np.random.seed(12345)\n",
        "\n",
        "    df_data         = process_csv(f'whole_brain_{conscious_state}.csv')\n",
        "    # load the data in the format of numpy but keep the 4D dimensions\n",
        "    BOLD_image      = load_fmri(f'whole_brain_{conscious_state}.nii.gz')\n",
        "    print(f'{conscious_state}\\tfMRI in {BOLD_image.shape} events in {df_data.shape}')\n",
        "    targets         = np.array([label_map[item] for item in df_data['targets']])[:,-1]\n",
        "    # get the image names in the order of the experimental trials\n",
        "    images          = df_data['paths'].apply(lambda x: x.split('.')[0] + '.npy').values\n",
        "    # get the CNN features (n_trial x 300)\n",
        "    CNN_feature     = np.array([np.load(os.path.join(feature_dir,\n",
        "                                                     model_name,\n",
        "                                                     item)) for item in images])\n",
        "    groups          = df_data['labels'].values\n",
        "    ############################################################################\n",
        "    ######## replace this part #################################################\n",
        "    # define a function to create the folds first\n",
        "    def _proc(df_data):\n",
        "        \"\"\"\n",
        "        This is useful when the number of folds are thousands\n",
        "        \"\"\"\n",
        "        df_picked = df_data.groupby('labels').apply(lambda x: x.sample(n = 1).drop('labels',axis = 1)).reset_index()\n",
        "        df_picked = df_picked.sort_values(['targets','subcategory','labels'])\n",
        "        idx_test  = df_picked['level_1'].values\n",
        "        return idx_test\n",
        "    print(f'partitioning data for {n_splits} folds')\n",
        "    idxs = Parallel(n_jobs = -1, verbose = 1)(delayed(_proc)(**{\n",
        "                'df_data':df_data,}) for _ in range(n_splits))\n",
        "    gc.collect() # free memory that is occupied by garbage\n",
        "    ############################################################################\n",
        "    ######## with this #########################################################\n",
        "    from sklearn.model_selection import StratifiedShuffleSplit\n",
        "    cv = StraitifiedShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 12345)\n",
        "    idxs_train,idxs_test = [],[]\n",
        "    for idx_train,idx_test in cv.split(df_data,targets):\n",
        "        idxs_train.append(idx_train)\n",
        "        idxs_test.append(idx_test)\n",
        "    ############################################################################\n",
        "    ##### end of modification No.1##############################################\n",
        "    ############################################################################\n",
        "    # define a function to run the RSA\n",
        "    def _searchligh_RSA(idx,\n",
        "                        sl_rad = radius, \n",
        "                        max_blk_edge = radius - 1, \n",
        "                        shape = Ball,\n",
        "                        min_active_voxels_proportion = 0,\n",
        "                        ):\n",
        "        # get the train,test split of a given fold\n",
        "        idx_train,idx_test = idx\n",
        "        # Brainiak function\n",
        "        sl = Searchlight(sl_rad = sl_rad, \n",
        "                        max_blk_edge = max_blk_edge, \n",
        "                        shape = shape,\n",
        "                        min_active_voxels_proportion = min_active_voxels_proportion,\n",
        "                        )\n",
        "        # distribute the data based on the sphere\n",
        "        ## the first input is usually the BOLD signal, and it is in the form of \n",
        "        ## lists not arrays, representing each subject\n",
        "        ## the second input is usually the mask, and it is in the form of array\n",
        "        sl.distribute([np.asanyarray(BOLD_image.dataobj)], \n",
        "                        np.asanyarray(load_fmri(whole_brain_mask).dataobj) == 1)\n",
        "        ########################################################################\n",
        "        ##### second modification ##############################################\n",
        "        # broadcasted data is the data that remains the same during RSA\n",
        "        sl.broadcast([targets,idx_train,idx_test]) # <-- add the indices of training and testing\n",
        "        ###### end of modification No.2 ########################################\n",
        "        # run searchlight algorithm\n",
        "        global_outputs = sl.run_searchlight(sfn,\n",
        "                                            pool_size = 1, # we run each RSA using a single CPU\n",
        "                                            )\n",
        "        return global_outputs\n",
        "    for _ in range(10):\n",
        "        gc.collect()\n",
        "    res = Parallel(n_jobs = -1,verbose = 1,)(delayed(_searchligh_RSA)(**{\n",
        "                'idx':idx}) for idx in idxs)\n",
        "    # save the data\n",
        "    results_to_save = np.zeros(np.concatenate([BOLD_image.shape[:3],[n_splits]]))\n",
        "    for ii,item in enumerate(res):\n",
        "        results_to_save[:,:,:,ii] = np.array(item, dtype=np.float)\n",
        "    results_to_save = new_img_like(BOLD_image,results_to_save,)\n",
        "    results_to_save.to_filename(f'searchlight_decoding_{conscious_state}.nii.gz')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}